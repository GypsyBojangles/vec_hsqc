#! /usr/bin/env python
"""General purpose methods related to newly imported data
and plotting thereof.
"""

def pickle_X_y( X, y ):
        import pickle
        with open('X_matrix.pickle', 'w') as f:
	    pickle.dump( X, f)
        with open('Y_vector.pickle', 'w') as f:
            pickle.dump( y, f)

def unpickle_X_y(  ):
        import pickle
        with open('X_matrix.pickle', 'r') as f:
            X = pickle.load(f)
        with open('Y_vector.pickle', 'r') as f:
            y = pickle.load(f)
        return (X, y)

def get_peak_indices( SP_obj, peaklist_ppm ):
    """Accepts as SpectrumPick instance object plus
    a peaklist supplied in ppm and converts to 
    index positions
    """
    index_pl = []
    for entry in peaklist_ppm:
	resid = entry[0]
	Nindex = SP_obj.uc0( str(entry[1]) + " ppm" )
	Hindex = SP_obj.uc1( str(entry[2]) + " ppm" )
	index_pl.append( [ resid, Nindex, Hindex ] )
    return index_pl


def plot_2D_peaks_assigned( data, picked_peaks, title ): 

        import nmrglue as ng
        import numpy as np
        import matplotlib.pyplot as plt
        import matplotlib.cm

        # plot parameters
        cmap = matplotlib.cm.Blues_r    # contour map (colors to use for contours)
        contour_start = 9.0e+06         # contour level start value
        contour_num = 20                # number of contour levels
        contour_factor = 1.20           # scaling factor between contour levels
        textsize = 6                    # text size of labels

        # calculate contour levels
        cl = contour_start * contour_factor ** np.arange(contour_num)

        # read in the data from a NMRPipe file
        #dic, data = ng.pipe.read("nmrpipe_2d/test.ft2")

        # read in the integration limits
        #peak_list = np.recfromtxt("limits.in")
        peak_list = [ [ b[0], b[2] - 10, b[1] -10, b[2] + 11, b[1] + 11] for b in picked_peaks]
        # create the figure
        fig = plt.figure()
        ax = fig.add_subplot(111)

        # plot the contours
        ax.contour(data, cl, cmap=cmap, 
                extent=(0, data.shape[1] - 1, 0, data.shape[0] - 1))

        # loop over the peaks
        for name, x0, y0, x1, y1 in peak_list:

            if x0 > x1:
                x0, x1 = x1, x0
            if y0 > y1:
                y0, y1 = y1, y0

            # plot a box around each peak and label
            ax.plot([x0, x1, x1, x0, x0], [y0, y0, y1, y1, y0], 'k')
            ax.text(x1 + 1, y0, name, size=textsize, color='r')
    
        # set limits
        #ax.set_xlim(1900, 2200)
        #ax.set_ylim(750, 1400)

        # save the figure
        fig.savefig( title + '.png')


def splist2pylist( peaklist_file , restype = True, N_H = True ):
        """Takes a Sparky peaklist and returns a python list (of lists).
        Python list returned is of the form:
        [<resnum (int)>, <N shift (float)>, <H shift (float)>]
        for each assigned residue
        """
        with open( peaklist_file, 'r' ) as f:
            all_list = [b.strip().split() for b in f if len(b) > 2 and 'Assignment' not in b]
        if restype and N_H:
	    all_list = [ [ int(b[0][1:-3]), float(b[1]), float(b[2]) ] for b in all_list[:]]
        elif restype:
	    all_list = [ [ int(b[0][1:]), float(b[1]), float(b[2]) ] for b in all_list[:]]
        elif N_H:
	    all_list = [ [ int(b[0][:-3]), float(b[1]), float(b[2]) ] for b in all_list[:]]
        return all_list

def import_peaklist( peaklist, peaklist_type = 'Sparky' ):
    """General purpose function for importing peaklists
    Initially for Sparky lists, can be extended to incluse others.
    """
    if peaklist_type == 'Sparky':
        ass_list = splist2pylist( peaklist )                 
        return ass_list


def find_nearest_rich( uc_list, data_array, data_dic, spectrum_name, control_spectrum_name, threshold = False, msep = (10,10), table = False, cluster = False ):
        """Takes a k X 3 list 'uc_list', which is a list of known assignments, eg
        a sparky peak list converted into a python list eg one row might be:
        ['F54N-H', '125.6541', '7.3456'] ie assignment, Nshift, Hshift.
        'data_array' and 'data_dic' are the nmrdataset matrix and dictionary respectively,
        eg those generated by a read function such as nmrglue.sparky.read( <filestring> )
        Picks peaks over the data using the specified threshold, then for each assignment,
        finds the closest peak with nonzero estimated linewidths is both dimensions.
        Returns both a list and dictionary of relevant info including distance from assigned
	position, linewidth and height.
        """
        import numpy as np, nmrglue as ng

        # get spectral parameters in a format we can use
        udic = ng.sparky.guess_udic( data_dic, data_array )

        x, y = np.shape( data_array )
        avgheight = sum( sum ( abs(data_array) ) ) / ( x * y ) #NOTE  abs included so that folded and unfolded spectra treated identically - is this valid??? 

        pick_threshold = avgheight
        if threshold:
	    pick_threshold = threshold

        uc0 = ng.sparky.make_uc(data_dic, data_array, dim=0)
        uc1 = ng.sparky.make_uc(data_dic, data_array, dim=1)

        # the below enables conversion of peak linewidths from datapoint units into Hz
        pt_2_Hz0 = data_dic['w1']['spectral_width'] / (data_dic['w1']['npoints'] - 1 )
        pt_2_Hz1 = data_dic['w2']['spectral_width'] / (data_dic['w2']['npoints'] - 1 )

        locations, lws, heights = ng.analysis.peakpick.pick(data_array, pick_threshold,
		msep = msep, table = table, cluster = cluster ) 
        # note that tuple 'msep' specifies array index spearations, not ppm differences

        lwhz = [ [ b[0] * pt_2_Hz0, b[1] * pt_2_Hz1 ] for b in lws ]

        foundpeaks = np.array( [ [ uc0.ppm(b[0]), uc1.ppm(b[1]) ] for b in locations ] )

        repos_dict = {}
        assigned = np.array( uc_list )
        scaling = np.array([0.15, 1.])
        j = 0 # counter for peaks with nonzero linewidths
        for i in range(len( foundpeaks )):
            peak = foundpeaks[i]
	    raw_distances = assigned[:,1:] - peak
	    distances = sum(abs( ( raw_distances ) * scaling ).T)
	    idx = distances.argmin()
	    mindistance =  distances[ idx ]
	    min_displacements = raw_distances[ idx ]
	    resid = uc_list[idx][0]
	    if min(lwhz[i]) > 0.0:
                j += 1
	    if resid not in repos_dict.keys() and min(lwhz[i]) > 0.0:
                repos_dict[ resid ] = { 'assignment' : uc_list[idx][0], 'Nshift' : peak[0],
		'Hshift' : peak[1], 'distance' : mindistance, 'lwN' : lwhz[i][0], 
		'lwH' : lwhz[i][1], 'height' : float(heights[i]), 'N_displacement' : 
		min_displacements[0], 'H_displacement' : min_displacements[1],
		'height_ave_ratio' : abs(heights[i]) / avgheight, 'exptID' : spectrum_name,
		'controlID' : control_spectrum_name, 'avgheight' : avgheight }
	    elif resid in repos_dict.keys() and repos_dict[ resid ]['distance'] > mindistance and min(lwhz[i]) > 0.0:
                repos_dict[ resid ] = { 'assignment' : uc_list[idx][0], 'Nshift' : peak[0],
		'Hshift' : peak[1], 'distance' : mindistance, 'lwN' : lwhz[i][0], 
		'lwH' : lwhz[i][1], 'height' : float(heights[i]), 'N_displacement' : 
		min_displacements[0], 'H_displacement' : min_displacements[1],
		'height_ave_ratio' : abs(heights[i]) / avgheight, 'exptID' : spectrum_name,
		'controlID' : control_spectrum_name, 'avgheight' : avgheight }
	    else:
	        pass
        n_peaks = j
        assigned_peaks = len( repos_dict.keys() )
        possible_peaks = (data_dic['w1']['size'] // msep[0]) * (data_dic['w2']['size'] // msep[1]) 
        details_dict = {'possible_peaks' : possible_peaks, 'found_peaks' : n_peaks, 'assigned_peaks' :
		assigned_peaks }

        return (repos_dict, details_dict)



def pick_spectrum( data_array, data_dic, spectrum_name, control_spectrum_name, threshold = False, msep = (10,10), table = False, cluster = False ):
        """Takes a k X 3 list 'uc_list', which is a list of known assignments, eg
        a sparky peak list converted into a python list eg one row might be:
        ['F54N-H', '125.6541', '7.3456'] ie assignment, Nshift, Hshift.
        'data_array' and 'data_dic' are the nmrdataset matrix and dictionary respectively,
        eg those generated by a read function such as nmrglue.sparky.read( <filestring> )
        Picks peaks over the data using the specified threshold, then for each assignment,
        finds the closest peak with nonzero estimated linewidths is both dimensions.
        Returns both a list and dictionary of relevant info including distance from assigned
	position, linewidth and height.
        """
        import numpy as np, nmrglue as ng

        # get spectral parameters in a format we can use
        udic = ng.sparky.guess_udic( data_dic, data_array )

        x, y = np.shape( data_array )
        avgheight = sum( sum ( abs(data_array) ) ) / ( x * y ) #NOTE  abs included so that folded and unfolded spectra treated identically - is this valid??? 

        pick_threshold = avgheight
        if threshold:
	    pick_threshold = threshold

        uc0 = ng.sparky.make_uc(data_dic, data_array, dim=0)
        uc1 = ng.sparky.make_uc(data_dic, data_array, dim=1)

        # the below enables conversion of peak linewidths from datapoint units into Hz
        pt_2_Hz0 = data_dic['w1']['spectral_width'] / (data_dic['w1']['npoints'] - 1 )
        pt_2_Hz1 = data_dic['w2']['spectral_width'] / (data_dic['w2']['npoints'] - 1 )

        locations, lws, heights = ng.analysis.peakpick.pick(data_array, pick_threshold,
		msep = msep, table = table, cluster = cluster ) 
        # note that tuple 'msep' specifies array index spearations, not ppm differences

        lwhz = [ [ b[0] * pt_2_Hz0, b[1] * pt_2_Hz1 ] for b in lws ]

        foundpeaks = np.array( [ [ uc0.ppm(b[0]), uc1.ppm(b[1]) ] for b in locations ] )

        repos_dict = {}
        scaling = np.array([0.15, 1.])
        j = 0 # counter for peaks with nonzero linewidths
        for i in range(len( foundpeaks )):
            peak = foundpeaks[i]
	    if min(lwhz[i]) > 0.0:
		j += 1
                repos_dict[ j ] = { 'Nshift' : peak[0],
		'Hshift' : peak[1], 'lwN' : lwhz[i][0], 
		'lwH' : lwhz[i][1], 'height' : float(heights[i]),
		'height_ave_ratio' : abs(heights[i]) / avgheight, 'exptID' : spectrum_name,
		'controlID' : control_spectrum_name, 'avgheight' : avgheight }
	    else:
	        pass
        n_peaks = j
        possible_peaks = (data_dic['w1']['size'] // msep[0]) * (data_dic['w2']['size'] // msep[1]) 
        details_dict = {'possible_peaks' : possible_peaks, 'found_peaks' : n_peaks }

        return (repos_dict, details_dict)

def find_nearest_assign( ass_list, SP_obj ):

        import numpy as np, os.path as opath
        peaks = SP_obj.picked_peaks
        lwhz = SP_obj.picked_lws
        heights = SP_obj.picked_heights
        avgheight = SP_obj.avgheight
        spectrum_name = opath.split( SP_obj.spectrum )[-1]
        control_spectrum_name = opath.split( SP_obj.control_spectrum )[-1]
        # the below nomenclature for control spectra is essential for downstream db steps
        if spectrum_name == control_spectrum_name:
            spectrum_name = 'control'
        assigned = np.array( ass_list )
        repos_dict = {}
        for i in range(len( peaks )):
            peak = peaks[i]
            raw_distances = assigned[:,1:] - peak
            distances = sum(abs( ( raw_distances ) * self.scaling ).T)
            idx = distances.argmin()
            mindistance =  distances[ idx ]
            min_displacements = raw_distances[ idx ]
            resid = ass_list[idx][0]
            if resid not in repos_dict.keys() and min(lwhz[i]) > 0.0:
                repos_dict[ resid ] = { 'assignment' : ass_list[idx][0], 'Nshift' : peak[0],
                'Hshift' : peak[1], 'distance' : mindistance, 'lwN' : lwhz[i][0],
                'lwH' : lwhz[i][1], 'height' : float(heights[i]), 'N_displacement' :
                min_displacements[0], 'H_displacement' : min_displacements[1],
                'height_ave_ratio' : abs(heights[i]) / avgheight, 'exptID' : spectrum_name,
                'controlID' : control_spectrum_name, 'avgheight' : avgheight }
            elif resid in repos_dict.keys() and repos_dict[ resid ]['distance'] > mindistance and min(lwhz[i]) > 0.0:
                repos_dict[ resid ] = { 'assignment' : ass_list[idx][0], 'Nshift' : peak[0],
                'Hshift' : peak[1], 'distance' : mindistance, 'lwN' : lwhz[i][0],
                'lwH' : lwhz[i][1], 'height' : float(heights[i]), 'N_displacement' :
                min_displacements[0], 'H_displacement' : min_displacements[1],
                'height_ave_ratio' : abs(heights[i]) / avgheight, 'exptID' : spectrum_name,
                'controlID' : control_spectrum_name, 'avgheight' : avgheight }
            else:
                pass
        assigned_peaks = len( repos_dict.keys() )
        setattr( SP_obj, 'assigned_peaks', assigned_peaks )

        return (repos_dict, assigned_peaks)

def extract_spectra_lists( filelist, spectrum_type = 'Sparky', peaklist_type = 'Sparky' ):


        if spectrum_type == 'Sparky':
            spectra = [ b for b in filelist if b[-5:] == '.ucsf']

        if peaklist_type == 'Sparky':
            peaklists = [ b for b in filelist if b[-5:] == '.list']


        return (spectra, peaklists)
